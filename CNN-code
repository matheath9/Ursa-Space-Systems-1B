# CNN Model
class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),  # 32 3x3x3 (RGB) filters applied to raw image set
            nn.MaxPool2d(kernel_size=2, stride=2),                                          # Pooling layer 2x2, (32,64,64) --> (32,32,32)
            nn.ReLU(),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1), # Output size = floor((W_in + 2P -K)/s) + 1
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.ReLU()
        )

        # Dynamically compute the flattened size
        with torch.no_grad():
            dummy_input = torch.zeros(1, 3, 75, 75)
            dummy_output = self.conv(dummy_input)
            self.flattened_size = dummy_output.view(1, -1).size(1)

        # Connects layers
        self.fc = nn.Sequential(
            nn.Linear(self.flattened_size, 64),  # Maps flattened images to 64-Dimensional hidden layer
            nn.ReLU(),                           # Activates non-linearity
            nn.Linear(64, 2)                     # Maps 64 features to two outputs per class
        )
    
    # Responsible for forward propagation
    def forward(self, x):
        x = self.conv(x)           # Passes images through CNN
        x = x.view(x.size(0), -1)  # Flattens each image into a 1D vector for the fully connected layer
        x = self.fc(x)             # Pass the flattened images through the fully connected layer
        return x
    
model = CNN()

# Store loss function and optimizer function
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=.01)






# Initialize Training data and testing data
trainloader = torch.utils.data.DataLoader(
    torch.utils.data.TensorDataset(X_train, y_train),
    batch_size=32,
    shuffle=True
)

testloader = torch.utils.data.DataLoader(
    torch.utils.data.TensorDataset(X_test, y_test),
    batch_size=32,
    shuffle=False
)






# Train the model

# Lists to store loss and accuracy for each epoch
loss_list = []
accuracy_list = []
precision_list = []
recall_list = []

# Iterate over all epochs
for epoch in range(40):
    epoch_loss = 0
    model.train()  # Set model to training mode

    for images, labels in trainloader:  # Iterate over batches
        outputs = model(images)             # Forward pass
        loss = criterion(outputs, labels)   # Compute loss

        optimizer.zero_grad()  # Clear old gradients
        loss.backward()        # Backpropagation
        optimizer.step()       # Update weights

        epoch_loss += loss.item()  # Accumulate batch loss

    # Compute average loss for the epoch
    avg_loss = epoch_loss / len(trainloader)
    loss_list.append(avg_loss)

    # Evaluation block (no gradient tracking)
    model.eval()  # Set model to evaluation mode
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in testloader:
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)
    
    accuracy_list.append(accuracy)
    precision_list.append(precision)
    recall_list.append(recall)
    print('Epoch[{}/40] Testing Accuracy {:.4f} | Loss {:.4f} | Precision {:.4f} | Recall {:.4f}'
          .format(epoch + 1, accuracy, avg_loss, precision, recall))
