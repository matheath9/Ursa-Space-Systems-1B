{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAR Iceberg Classifier - MLP Baseline Model\n",
        "\n",
        "**Author:** Colin\n",
        "**Date:** October 2025\n",
        "**Project:** Fall AI Studio Challenge with Ursa Space Systems  \n",
        "**Dataset:** Statoil/C-CORE Iceberg Classifier Challenge\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Overview\n",
        "\n",
        "This notebook contains my standalone Multi-Layer Perceptron (MLP) implementation for classifying SAR (Synthetic Aperture Radar) imagery to distinguish ships from icebergs.\n",
        "\n",
        "### Model Architecture\n",
        "- **Input:** 11,250 features (75Ã—75Ã—2 flattened SAR bands)\n",
        "- **Hidden Layers:** [512, 256, 128] neurons with ReLU activation\n",
        "- **Output:** 1 neuron with sigmoid activation\n",
        "- **Optimizer:** Adam (learning_rate=0.001)\n",
        "- **Regularization:** L2 (alpha=0.0001)\n",
        "- **Early Stopping:** Enabled (patience=10)\n",
        "\n",
        "### Performance\n",
        "- **Validation ROC-AUC:** ~0.985\n",
        "- **Training Time:** 5-10 minutes (CPU)\n",
        "- **Iterations:** 50-100 epochs (with early stopping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed\n",
        "# !pip install pandas numpy matplotlib seaborn scikit-learn joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    accuracy_score\n",
        ")\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n",
        "print(\"   Ready to build MLP iceberg classifier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Update the file paths below to point to your local data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - UPDATE THESE PATHS!\n",
        "# ============================================================================\n",
        "\n",
        "TRAIN_PATH = 'train.json'  # â† Update this path\n",
        "TEST_PATH = 'test.json'    # â† Update this path\n",
        "\n",
        "# Model hyperparameters\n",
        "HIDDEN_LAYERS = (512, 256, 128)  # Hidden layer sizes\n",
        "LEARNING_RATE = 0.001            # Initial learning rate\n",
        "MAX_ITERATIONS = 100             # Maximum training epochs\n",
        "ALPHA = 0.0001                   # L2 regularization strength\n",
        "VALIDATION_SPLIT = 0.2           # Fraction for validation set\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Training data: {TRAIN_PATH}\")\n",
        "print(f\"  Test data: {TEST_PATH}\")\n",
        "print(f\"  Hidden layers: {HIDDEN_LAYERS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Max iterations: {MAX_ITERATIONS}\")\n",
        "print(f\"  L2 alpha: {ALPHA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load training and test data\n",
        "train_data = pd.read_json(TRAIN_PATH)\n",
        "test_data = pd.read_json(TEST_PATH)\n",
        "\n",
        "print(f\"\\nâœ… Training data loaded: {train_data.shape}\")\n",
        "print(f\"âœ… Test data loaded: {test_data.shape}\")\n",
        "\n",
        "# Display class distribution\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "class_counts = train_data['is_iceberg'].value_counts()\n",
        "print(f\"  Ships (0): {class_counts[0]} ({class_counts[0]/len(train_data)*100:.1f}%)\")\n",
        "print(f\"  Icebergs (1): {class_counts[1]} ({class_counts[1]/len(train_data)*100:.1f}%)\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\nFirst few rows of training data:\")\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing\n",
        "\n",
        "Convert SAR imagery into feature vectors suitable for the MLP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_features(data, has_labels=True):\n",
        "    \"\"\"\n",
        "    Prepare SAR imagery data for MLP input.\n",
        "    \n",
        "    Flattens the two 75x75 SAR bands into a single feature vector.\n",
        "    \n",
        "    Args:\n",
        "        data: DataFrame with 'band_1' and 'band_2' columns\n",
        "        has_labels: Whether data includes 'is_iceberg' labels\n",
        "        \n",
        "    Returns:\n",
        "        X: Feature matrix (n_samples, 11250)\n",
        "        y: Labels (if has_labels=True), None otherwise\n",
        "    \"\"\"\n",
        "    # Flatten each band from 75x75 to 5625\n",
        "    band1 = np.array([np.array(band).flatten() for band in data['band_1']])\n",
        "    band2 = np.array([np.array(band).flatten() for band in data['band_2']])\n",
        "    \n",
        "    # Concatenate both bands: 5625 + 5625 = 11,250 features\n",
        "    X = np.concatenate([band1, band2], axis=1)\n",
        "    \n",
        "    if has_labels:\n",
        "        y = data['is_iceberg'].values\n",
        "        return X, y\n",
        "    else:\n",
        "        return X, None\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PREPROCESSING DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Prepare training features and labels\n",
        "print(\"\\nPreparing training data...\")\n",
        "X_full, y_full = prepare_features(train_data, has_labels=True)\n",
        "print(f\"âœ… Feature matrix shape: {X_full.shape}\")\n",
        "print(f\"   ({X_full.shape[1]} features = 75Ã—75 pixels Ã— 2 bands)\")\n",
        "\n",
        "# Prepare test features\n",
        "print(\"\\nPreparing test data...\")\n",
        "X_test_full, _ = prepare_features(test_data, has_labels=False)\n",
        "print(f\"âœ… Test feature matrix shape: {X_test_full.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full,\n",
        "    test_size=VALIDATION_SPLIT,\n",
        "    random_state=42,\n",
        "    stratify=y_full  # Maintain class balance\n",
        ")\n",
        "\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"  Training samples: {X_train.shape[0]}\")\n",
        "print(f\"  Validation samples: {X_val.shape[0]}\")\n",
        "print(f\"  Test samples: {X_test_full.shape[0]}\")\n",
        "\n",
        "# Verify class balance\n",
        "train_balance = np.bincount(y_train) / len(y_train) * 100\n",
        "val_balance = np.bincount(y_val) / len(y_val) * 100\n",
        "\n",
        "print(f\"\\nClass balance:\")\n",
        "print(f\"  Training - Ships: {train_balance[0]:.1f}%, Icebergs: {train_balance[1]:.1f}%\")\n",
        "print(f\"  Validation - Ships: {val_balance[0]:.1f}%, Icebergs: {val_balance[1]:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Standardization\n",
        "\n",
        "Neural networks require normalized features for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nStandardizing features...\")\n",
        "\n",
        "# Initialize and fit scaler on training data only\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test_full)\n",
        "\n",
        "print(\"âœ… Features standardized (mean=0, std=1)\")\n",
        "\n",
        "# Verify standardization\n",
        "print(f\"\\nTraining set statistics:\")\n",
        "print(f\"  Mean: {X_train_scaled.mean():.6f}\")\n",
        "print(f\"  Std: {X_train_scaled.std():.6f}\")\n",
        "print(f\"  Min: {X_train_scaled.min():.2f}\")\n",
        "print(f\"  Max: {X_train_scaled.max():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build MLP Model\n",
        "\n",
        "Configure the Multi-Layer Perceptron architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"BUILDING MLP MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize MLP classifier\n",
        "mlp_model = MLPClassifier(\n",
        "    hidden_layer_sizes=HIDDEN_LAYERS,\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=ALPHA,\n",
        "    batch_size=32,\n",
        "    learning_rate_init=LEARNING_RATE,\n",
        "    max_iter=MAX_ITERATIONS,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1,\n",
        "    n_iter_no_change=10,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Display architecture\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Input Layer:    {X_full.shape[1]:>6} neurons\")\n",
        "for i, layer_size in enumerate(HIDDEN_LAYERS, 1):\n",
        "    print(f\"  Hidden Layer {i}: {layer_size:>6} neurons (ReLU)\")\n",
        "print(f\"  Output Layer:   {1:>6} neuron  (Sigmoid)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\nTraining Configuration:\")\n",
        "print(f\"  Optimizer: Adam\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Batch size: 32\")\n",
        "print(f\"  L2 regularization: {ALPHA}\")\n",
        "print(f\"  Early stopping: Enabled (patience=10)\")\n",
        "print(f\"  Max iterations: {MAX_ITERATIONS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train the Model\n",
        "\n",
        "This may take 5-10 minutes depending on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING MLP MODEL\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Train the model\n",
        "mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nâœ… Training finished successfully!\")\n",
        "print(f\"   Total iterations: {mlp_model.n_iter_}\")\n",
        "print(f\"   Final loss: {mlp_model.loss_:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate predictions\n",
        "print(\"\\nGenerating predictions...\")\n",
        "train_pred = mlp_model.predict_proba(X_train_scaled)[:, 1]\n",
        "val_pred = mlp_model.predict_proba(X_val_scaled)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC scores\n",
        "train_auc = roc_auc_score(y_train, train_pred)\n",
        "val_auc = roc_auc_score(y_val, val_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PERFORMANCE METRICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n  Training ROC-AUC:   {train_auc:.4f}\")\n",
        "print(f\"  Validation ROC-AUC: {val_auc:.4f}\")\n",
        "print(f\"  Overfitting gap:    {train_auc - val_auc:.4f}\")\n",
        "\n",
        "# Binary predictions for classification report\n",
        "val_pred_binary = (val_pred > 0.5).astype(int)\n",
        "val_accuracy = accuracy_score(y_val, val_pred_binary)\n",
        "\n",
        "print(f\"\\n  Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLASSIFICATION REPORT (Validation Set)\")\n",
        "print(\"=\"*70)\n",
        "print(classification_report(y_val, val_pred_binary,\n",
        "                          target_names=['Ship', 'Iceberg'],\n",
        "                          digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loss Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(mlp_model.loss_curve_, linewidth=2.5, color='#2E86AB')\n",
        "plt.xlabel('Iteration', fontsize=13)\n",
        "plt.ylabel('Loss', fontsize=13)\n",
        "plt.title('MLP Training Loss Curve', fontsize=15, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('mlp_training_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Training curve saved to 'mlp_training_curve.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_val, val_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(fpr, tpr, linewidth=2.5, color='#A23B72',\n",
        "         label=f'MLP (AUC = {val_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier')\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=13)\n",
        "plt.ylabel('True Positive Rate', fontsize=13)\n",
        "plt.title('ROC Curve - MLP Iceberg Classifier', fontsize=15, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('mlp_roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… ROC curve saved to 'mlp_roc_curve.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_val, val_pred_binary)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "           cbar_kws={'label': 'Count'},\n",
        "           annot_kws={'size': 14})\n",
        "plt.xlabel('Predicted Label', fontsize=13)\n",
        "plt.ylabel('True Label', fontsize=13)\n",
        "plt.title('Confusion Matrix - MLP Iceberg Classifier',\n",
        "         fontsize=15, fontweight='bold')\n",
        "plt.xticks([0.5, 1.5], ['Ship', 'Iceberg'])\n",
        "plt.yticks([0.5, 1.5], ['Ship', 'Iceberg'])\n",
        "plt.tight_layout()\n",
        "plt.savefig('mlp_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Confusion matrix saved to 'mlp_confusion_matrix.png'\")\n",
        "\n",
        "# Print confusion matrix breakdown\n",
        "print(\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"  True Negatives (Ship â†’ Ship):       {cm[0,0]}\")\n",
        "print(f\"  False Positives (Ship â†’ Iceberg):   {cm[0,1]}\")\n",
        "print(f\"  False Negatives (Iceberg â†’ Ship):   {cm[1,0]}\")\n",
        "print(f\"  True Positives (Iceberg â†’ Iceberg): {cm[1,1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision-Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate precision-recall curve\n",
        "precision, recall, pr_thresholds = precision_recall_curve(y_val, val_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(recall, precision, linewidth=2.5, color='#F18F01')\n",
        "plt.xlabel('Recall', fontsize=13)\n",
        "plt.ylabel('Precision', fontsize=13)\n",
        "plt.title('Precision-Recall Curve - MLP Iceberg Classifier',\n",
        "         fontsize=15, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('mlp_precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Precision-Recall curve saved to 'mlp_precision_recall_curve.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"SAVING MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save trained model\n",
        "model_path = 'mlp_iceberg_model.pkl'\n",
        "model_data = {\n",
        "    'model': mlp_model,\n",
        "    'scaler': scaler,\n",
        "    'hidden_layers': HIDDEN_LAYERS,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'alpha': ALPHA,\n",
        "    'train_auc': train_auc,\n",
        "    'val_auc': val_auc\n",
        "}\n",
        "\n",
        "joblib.dump(model_data, model_path)\n",
        "print(f\"\\nâœ… Model saved to '{model_path}'\")\n",
        "\n",
        "# Also save scaler separately for convenience\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(f\"âœ… Scaler saved to 'scaler.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Generate Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"GENERATING TEST PREDICTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate predictions on test set\n",
        "print(\"\\nGenerating predictions for test set...\")\n",
        "test_pred = mlp_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(f\"âœ… Generated {len(test_pred)} predictions\")\n",
        "print(f\"\\nPrediction statistics:\")\n",
        "print(f\"  Mean: {test_pred.mean():.4f}\")\n",
        "print(f\"  Std:  {test_pred.std():.4f}\")\n",
        "print(f\"  Min:  {test_pred.min():.4f}\")\n",
        "print(f\"  Max:  {test_pred.max():.4f}\")\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'is_iceberg': test_pred\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission_path = 'submission_mlp_colin.csv'\n",
        "submission.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Kaggle submission saved to '{submission_path}'\")\n",
        "print(\"\\nFirst few predictions:\")\n",
        "print(submission.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nâœ… MLP Training Complete!\\n\")\n",
        "\n",
        "print(\"Model Architecture:\")\n",
        "print(f\"  Hidden layers: {HIDDEN_LAYERS}\")\n",
        "print(f\"  Total parameters: ~{sum([512*11250, 512*256, 256*128, 128*1]):,}\")\n",
        "\n",
        "print(\"\\nPerformance:\")\n",
        "print(f\"  Training ROC-AUC:   {train_auc:.4f}\")\n",
        "print(f\"  Validation ROC-AUC: {val_auc:.4f}\")\n",
        "print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nTraining Details:\")\n",
        "print(f\"  Iterations: {mlp_model.n_iter_}\")\n",
        "print(f\"  Final loss: {mlp_model.loss_:.6f}\")\n",
        "\n",
        "print(\"\\nGenerated Files:\")\n",
        "print(\"  ðŸ“Š mlp_training_curve.png\")\n",
        "print(\"  ðŸ“Š mlp_roc_curve.png\")\n",
        "print(\"  ðŸ“Š mlp_confusion_matrix.png\")\n",
        "print(\"  ðŸ“Š mlp_precision_recall_curve.png\")\n",
        "print(\"  ðŸ’¾ mlp_iceberg_model.pkl\")\n",
        "print(\"  ðŸ’¾ scaler.pkl\")\n",
        "print(\"  ðŸ“ submission_mlp_colin.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Next Steps:\")\n",
        "print(\"  1. Submit 'submission_mlp_colin.csv' to Kaggle\")\n",
        "print(\"  2. Compare results with other models\")\n",
        "print(\"  3. Experiment with hyperparameters\")\n",
        "print(\"  4. Try different architectures\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Optional: Load and Test Saved Model\n",
        "\n",
        "Run this section to verify that the saved model can be loaded and used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "print(\"Testing model loading...\\n\")\n",
        "loaded_model_data = joblib.load('mlp_iceberg_model.pkl')\n",
        "\n",
        "loaded_model = loaded_model_data['model']\n",
        "loaded_scaler = loaded_model_data['scaler']\n",
        "\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "print(\"\\nModel configuration:\")\n",
        "print(f\"  Hidden layers: {loaded_model_data['hidden_layers']}\")\n",
        "print(f\"  Learning rate: {loaded_model_data['learning_rate']}\")\n",
        "print(f\"  L2 alpha: {loaded_model_data['alpha']}\")\n",
        "print(f\"  Training AUC: {loaded_model_data['train_auc']:.4f}\")\n",
        "print(f\"  Validation AUC: {loaded_model_data['val_auc']:.4f}\")\n",
        "\n",
        "# Test prediction on a sample\n",
        "sample_features = X_val_scaled[:5]\n",
        "sample_predictions = loaded_model.predict_proba(sample_features)[:, 1]\n",
        "\n",
        "print(\"\\nSample predictions:\")\n",
        "for i, pred in enumerate(sample_predictions):\n",
        "    true_label = 'Iceberg' if y_val[i] == 1 else 'Ship'\n",
        "    pred_label = 'Iceberg' if pred > 0.5 else 'Ship'\n",
        "    print(f\"  Sample {i+1}: {pred:.4f} â†’ {pred_label} (True: {true_label})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Optional: Hyperparameter Experimentation\n",
        "\n",
        "Uncomment and modify this section to experiment with different architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Experiment with different architectures\n",
        "# architectures_to_try = [\n",
        "#     (256, 128),           # Smaller network\n",
        "#     (512, 256, 128),      # Original (current best)\n",
        "#     (1024, 512, 256, 128),# Deeper network\n",
        "#     (512, 512, 512),      # Wider network\n",
        "# ]\n",
        "\n",
        "# results = []\n",
        "\n",
        "# for arch in architectures_to_try:\n",
        "#     print(f\"\\nTrying architecture: {arch}\")\n",
        "    \n",
        "#     model = MLPClassifier(\n",
        "#         hidden_layer_sizes=arch,\n",
        "#         activation='relu',\n",
        "#         solver='adam',\n",
        "#         alpha=0.0001,\n",
        "#         batch_size=32,\n",
        "#         learning_rate_init=0.001,\n",
        "#         max_iter=100,\n",
        "#         random_state=42,\n",
        "#         early_stopping=True,\n",
        "#         validation_fraction=0.1,\n",
        "#         n_iter_no_change=10,\n",
        "#         verbose=False\n",
        "#     )\n",
        "    \n",
        "#     model.fit(X_train_scaled, y_train)\n",
        "#     val_pred = model.predict_proba(X_val_scaled)[:, 1]\n",
        "#     val_auc = roc_auc_score(y_val, val_pred)\n",
        "    \n",
        "#     results.append({\n",
        "#         'architecture': str(arch),\n",
        "#         'val_auc': val_auc,\n",
        "#         'iterations': model.n_iter_\n",
        "#     })\n",
        "    \n",
        "#     print(f\"  Val AUC: {val_auc:.4f}, Iterations: {model.n_iter_}\")\n",
        "\n",
        "# # Display results\n",
        "# results_df = pd.DataFrame(results)\n",
        "# results_df = results_df.sort_values('val_auc', ascending=False)\n",
        "# print(\"\\nArchitecture Comparison:\")\n",
        "# print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**End of Notebook**\n",
        "\n",
        "Author: Colin  \n",
        "Project: Fall AI Studio Challenge with Ursa Space Systems  \n",
        "Date: October 2025"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
